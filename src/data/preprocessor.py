"""
äº¤é€šæ•°æ®é¢„å¤„ç†å™¨
æ•°æ®æ¸…æ´—ã€å½’ä¸€åŒ–ã€ç‰¹å¾å·¥ç¨‹
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
from typing import Dict, Tuple, Optional
import pickle
from pathlib import Path
import sys

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.utils.config import config


class TrafficDataPreprocessor:
    """äº¤é€šæ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–é¢„å¤„ç†å™¨"""
        self.scaler = None
        self.scaler_type = None
    
    def handle_missing_values(
        self,
        data: np.ndarray,
        method: str = 'interpolate'
    ) -> np.ndarray:
        """
        å¤„ç†ç¼ºå¤±å€¼
        
        Args:
            data: è¾“å…¥æ•°æ® (timesteps, sensors)
            method: å¤„ç†æ–¹æ³• ('interpolate', 'forward_fill', 'backward_fill', 'mean')
        
        Returns:
            å¤„ç†åçš„æ•°æ®
        """
        print(f"ğŸ”§ å¤„ç†ç¼ºå¤±å€¼ (æ–¹æ³•: {method})...")
        
        # ç»Ÿè®¡ç¼ºå¤±å€¼
        nan_count = np.isnan(data).sum()
        if nan_count == 0:
            print(f"   âœ“ æ— ç¼ºå¤±å€¼")
            return data
        
        print(f"   å‘ç°ç¼ºå¤±å€¼: {nan_count} ({nan_count/data.size*100:.2f}%)")
        
        df = pd.DataFrame(data)
        
        if method == 'interpolate':
            # çº¿æ€§æ’å€¼
            df = df.interpolate(method='linear', axis=0, limit_direction='both')
        elif method == 'forward_fill':
            # å‰å‘å¡«å……
            df = df.fillna(method='ffill')
        elif method == 'backward_fill':
            # åå‘å¡«å……
            df = df.fillna(method='bfill')
        elif method == 'mean':
            # å‡å€¼å¡«å……
            df = df.fillna(df.mean())
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {method}")
        
        # å¦‚æœè¿˜æœ‰ç¼ºå¤±å€¼ï¼Œä½¿ç”¨0å¡«å……
        df = df.fillna(0)
        
        result = df.values
        print(f"   âœ“ ç¼ºå¤±å€¼å¤„ç†å®Œæˆ")
        
        return result
    
    def detect_outliers(
        self,
        data: np.ndarray,
        method: str = 'iqr',
        threshold: float = 1.5
    ) -> np.ndarray:
        """
        æ£€æµ‹å¼‚å¸¸å€¼
        
        Args:
            data: è¾“å…¥æ•°æ®
            method: æ£€æµ‹æ–¹æ³• ('iqr', 'zscore')
            threshold: é˜ˆå€¼
        
        Returns:
            å¼‚å¸¸å€¼æ©ç ï¼ˆTrueè¡¨ç¤ºå¼‚å¸¸ï¼‰
        """
        print(f"ğŸ” æ£€æµ‹å¼‚å¸¸å€¼ (æ–¹æ³•: {method}, é˜ˆå€¼: {threshold})...")
        
        if method == 'iqr':
            # IQRæ–¹æ³•
            Q1 = np.percentile(data, 25, axis=0)
            Q3 = np.percentile(data, 75, axis=0)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - threshold * IQR
            upper_bound = Q3 + threshold * IQR
            
            outliers = (data < lower_bound) | (data > upper_bound)
        
        elif method == 'zscore':
            # Z-scoreæ–¹æ³•
            mean = np.mean(data, axis=0)
            std = np.std(data, axis=0)
            z_scores = np.abs((data - mean) / (std + 1e-8))
            
            outliers = z_scores > threshold
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {method}")
        
        outlier_count = outliers.sum()
        print(f"   æ£€æµ‹åˆ°å¼‚å¸¸å€¼: {outlier_count} ({outlier_count/data.size*100:.2f}%)")
        
        return outliers
    
    def handle_outliers(
        self,
        data: np.ndarray,
        action: str = 'clip',
        method: str = 'iqr',
        threshold: float = 1.5
    ) -> np.ndarray:
        """
        å¤„ç†å¼‚å¸¸å€¼
        
        Args:
            data: è¾“å…¥æ•°æ®
            action: å¤„ç†åŠ¨ä½œ ('clip', 'remove', 'interpolate')
            method: æ£€æµ‹æ–¹æ³•
            threshold: é˜ˆå€¼
        
        Returns:
            å¤„ç†åçš„æ•°æ®
        """
        outliers = self.detect_outliers(data, method, threshold)
        
        if action == 'clip':
            # è£å‰ªåˆ°åˆç†èŒƒå›´
            Q1 = np.percentile(data, 25, axis=0)
            Q3 = np.percentile(data, 75, axis=0)
            IQR = Q3 - Q1
            lower_bound = Q1 - threshold * IQR
            upper_bound = Q3 + threshold * IQR
            
            data = np.clip(data, lower_bound, upper_bound)
            print(f"   âœ“ å¼‚å¸¸å€¼å·²è£å‰ª")
        
        elif action == 'interpolate':
            # ä½¿ç”¨æ’å€¼æ›¿æ¢
            data_copy = data.copy()
            data_copy[outliers] = np.nan
            data = self.handle_missing_values(data_copy, method='interpolate')
            print(f"   âœ“ å¼‚å¸¸å€¼å·²æ’å€¼")
        
        elif action == 'remove':
            print(f"   âš ï¸ 'remove'åŠ¨ä½œæš‚ä¸æ”¯æŒï¼Œä½¿ç”¨'clip'æ›¿ä»£")
            return self.handle_outliers(data, action='clip', method=method, threshold=threshold)
        
        return data
    
    def normalize(
        self,
        data: np.ndarray,
        method: str = 'minmax',
        feature_range: Tuple[float, float] = (0, 1)
    ) -> np.ndarray:
        """
        æ•°æ®å½’ä¸€åŒ–
        
        Args:
            data: è¾“å…¥æ•°æ® (timesteps, sensors)
            method: å½’ä¸€åŒ–æ–¹æ³• ('minmax', 'standard', 'robust')
            feature_range: MinMaxScalerçš„èŒƒå›´
        
        Returns:
            å½’ä¸€åŒ–åçš„æ•°æ®
        """
        print(f"ğŸ“Š æ•°æ®å½’ä¸€åŒ– (æ–¹æ³•: {method})...")
        
        self.scaler_type = method
        
        # é‡å¡‘æ•°æ®ä»¥é€‚åº”scaler
        original_shape = data.shape
        data_reshaped = data.reshape(-1, 1) if data.ndim == 2 else data
        
        if method == 'minmax':
            self.scaler = MinMaxScaler(feature_range=feature_range)
        elif method == 'standard':
            self.scaler = StandardScaler()
        elif method == 'robust':
            self.scaler = RobustScaler()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å½’ä¸€åŒ–æ–¹æ³•: {method}")
        
        # å¯¹æ¯ä¸ªä¼ æ„Ÿå™¨ç‹¬ç«‹å½’ä¸€åŒ–
        normalized = np.zeros_like(data)
        for i in range(data.shape[1]):
            sensor_data = data[:, i].reshape(-1, 1)
            normalized[:, i] = self.scaler.fit_transform(sensor_data).flatten()
        
        print(f"   âœ“ å½’ä¸€åŒ–å®Œæˆ")
        print(f"   åŸå§‹èŒƒå›´: [{data.min():.2f}, {data.max():.2f}]")
        print(f"   å½’ä¸€åŒ–èŒƒå›´: [{normalized.min():.2f}, {normalized.max():.2f}]")
        
        return normalized
    
    def inverse_transform(self, data: np.ndarray) -> np.ndarray:
        """
        åå½’ä¸€åŒ–
        
        Args:
            data: å½’ä¸€åŒ–åçš„æ•°æ®
        
        Returns:
            åŸå§‹å°ºåº¦çš„æ•°æ®
        """
        if self.scaler is None:
            raise ValueError("å°šæœªè®­ç»ƒscalerï¼Œè¯·å…ˆè°ƒç”¨normalize()")
        
        # å¯¹æ¯ä¸ªä¼ æ„Ÿå™¨ç‹¬ç«‹åå½’ä¸€åŒ–
        denormalized = np.zeros_like(data)
        for i in range(data.shape[1] if data.ndim > 1 else 1):
            if data.ndim > 1:
                sensor_data = data[:, i].reshape(-1, 1)
                denormalized[:, i] = self.scaler.inverse_transform(sensor_data).flatten()
            else:
                denormalized = self.scaler.inverse_transform(data.reshape(-1, 1)).flatten()
        
        return denormalized
    
    def create_congestion_labels(
        self,
        speed: np.ndarray,
        thresholds: Dict[str, float] = None
    ) -> np.ndarray:
        """
        åˆ›å»ºæ‹¥å µçŠ¶æ€æ ‡ç­¾
        
        Args:
            speed: é€Ÿåº¦æ•°æ® (timesteps, sensors)
            thresholds: é˜ˆå€¼å­—å…¸ {'clear': 60, 'slow': 30}
        
        Returns:
            æ‹¥å µæ ‡ç­¾ (0: ç•…é€š, 1: ç¼“è¡Œ, 2: æ‹¥å µ)
        """
        print(f"ğŸš¦ ç”Ÿæˆæ‹¥å µçŠ¶æ€æ ‡ç­¾...")
        
        if thresholds is None:
            thresholds = {'clear': 60, 'slow': 30}
        
        labels = np.zeros_like(speed, dtype=int)
        
        # ç•…é€š: é€Ÿåº¦ > 60
        labels[speed > thresholds['clear']] = 0
        
        # ç¼“è¡Œ: 30 <= é€Ÿåº¦ <= 60
        labels[(speed >= thresholds['slow']) & (speed <= thresholds['clear'])] = 1
        
        # æ‹¥å µ: é€Ÿåº¦ < 30
        labels[speed < thresholds['slow']] = 2
        
        # ç»Ÿè®¡å„ç±»åˆ«æ•°é‡
        unique, counts = np.unique(labels, return_counts=True)
        label_names = ['ç•…é€š', 'ç¼“è¡Œ', 'æ‹¥å µ']
        
        print(f"   æ ‡ç­¾åˆ†å¸ƒ:")
        for label, count in zip(unique, counts):
            percentage = count / labels.size * 100
            print(f"     {label_names[label]}: {count} ({percentage:.1f}%)")
        
        return labels
    
    def save_scaler(self, filepath: str):
        """
        ä¿å­˜å½’ä¸€åŒ–å™¨
        
        Args:
            filepath: ä¿å­˜è·¯å¾„
        """
        if self.scaler is None:
            raise ValueError("å°šæœªè®­ç»ƒscaler")
        
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        with open(filepath, 'wb') as f:
            pickle.dump({
                'scaler': self.scaler,
                'scaler_type': self.scaler_type
            }, f)
        
        print(f"âœ… Scalerå·²ä¿å­˜: {filepath}")
    
    def load_scaler(self, filepath: str):
        """
        åŠ è½½å½’ä¸€åŒ–å™¨
        
        Args:
            filepath: æ–‡ä»¶è·¯å¾„
        """
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
            self.scaler = data['scaler']
            self.scaler_type = data['scaler_type']
        
        print(f"âœ… Scalerå·²åŠ è½½: {filepath}")
    
    def process_data(
        self,
        data: Dict[str, np.ndarray],
        save_scaler: bool = True
    ) -> Dict[str, np.ndarray]:
        """
        å®Œæ•´çš„æ•°æ®é¢„å¤„ç†æµç¨‹
        
        Args:
            data: åŒ…å«flow, speed, occupancyçš„å­—å…¸
            save_scaler: æ˜¯å¦ä¿å­˜scaler
        
        Returns:
            å¤„ç†åçš„æ•°æ®
        """
        print("\n" + "=" * 60)
        print("å¼€å§‹æ•°æ®é¢„å¤„ç†")
        print("=" * 60)
        
        processed = {}
        
        # å¤„ç†æ¯ä¸ªç‰¹å¾
        for key in ['flow', 'speed', 'occupancy']:
            if key not in data:
                continue
            
            print(f"\nå¤„ç†ç‰¹å¾: {key}")
            print("-" * 60)
            
            # 1. å¤„ç†ç¼ºå¤±å€¼
            clean_data = self.handle_missing_values(data[key])
            
            # 2. å¤„ç†å¼‚å¸¸å€¼
            clean_data = self.handle_outliers(clean_data)
            
            # 3. å½’ä¸€åŒ–
            normalized = self.normalize(clean_data)
            
            processed[key] = normalized
        
        # 4. åˆ›å»ºæ‹¥å µæ ‡ç­¾
        if 'speed' in data:
            processed['congestion'] = self.create_congestion_labels(data['speed'])
        
        # 5. ä¿å­˜scaler
        if save_scaler:
            scaler_path = config.get('paths.data_processed') + 'scaler.pkl'
            self.save_scaler(scaler_path)
        
        print("\n" + "=" * 60)
        print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ!")
        print("=" * 60)
        
        return processed


if __name__ == "__main__":
    # æµ‹è¯•é¢„å¤„ç†å™¨
    print("æµ‹è¯•æ•°æ®é¢„å¤„ç†å™¨...")
    
    from src.data.loader import TrafficDataLoader
    
    # åŠ è½½æ•°æ®
    loader = TrafficDataLoader()
    data = loader.load_data()
    
    # åˆ›å»ºé¢„å¤„ç†å™¨
    preprocessor = TrafficDataPreprocessor()
    
    # å¤„ç†æ•°æ®
    processed = preprocessor.process_data(data)
    
    print(f"\nå¤„ç†åçš„æ•°æ®:")
    for key, value in processed.items():
        print(f"  {key}: {value.shape}")

